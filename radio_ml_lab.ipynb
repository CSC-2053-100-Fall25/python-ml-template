{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab: Predicting FM Radio Signal Strength\n",
    "## CSC 2053 - Introduction to Regression\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "1. Understand the basic ML workflow: data â†’ features â†’ model â†’ evaluation\n",
    "2. Build and evaluate regression models using scikit-learn\n",
    "3. Compare model performance using metrics like RÂ² and RMSE\n",
    "4. Practice feature engineering to improve predictions\n",
    "\n",
    "### The Problem\n",
    "We have RadioLand FM radio station data from 25 US cities. Each row represents a station that was receivable at a specific location. We want to **predict the field strength** (just how strong that signal will be) a station will have at a given location based on technical parameters like:\n",
    "- Distance from transmitter to receiver\n",
    "- Transmitter power (ERP)\n",
    "- Frequency\n",
    "- Antenna height\n",
    "\n",
    "This is a **regression** problem - we're predicting a continuous number (field strength in dBu).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "First, let's import the tools we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Our dataset contains FM stations receivable from 25 diverse US locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV from GitHub\n",
    "url = \"https://raw.githubusercontent.com/CSC-2053-100-Fall25/python-ml-template/main/fm_stations_25_locations_ml_dataset.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "Let's understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What columns do we have?\n",
    "print(\"Key columns for our analysis:\")\n",
    "key_cols = ['callsign', 'frequency', 'distance', 'field_strength', 'erp', 'haat', 'class', 'search_location']\n",
    "print(df[key_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary of Key Numeric Features:\")\n",
    "df[['distance', 'field_strength', 'erp', 'frequency', 'haat']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in our key columns\n",
    "print(\"\\nMissing values in key columns:\")\n",
    "print(df[['distance', 'field_strength', 'erp', 'frequency', 'haat']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Cleaning\n\n**Before we can build models, we need clean data!**\n\n### Why Data Cleaning Matters\n\n**\"Garbage in, garbage out\"** - This is a fundamental principle in data science. ML models can only learn from the data you give them. If your data has problems, your model will have problems too.\n\nThink of it like cooking: You can't make a great meal with spoiled ingredients, no matter how skilled the chef!\n\n### Common Data Quality Issues\n\nReal-world datasets often have:\n- **Missing values** (NaN, null, empty cells)\n- **Invalid values** (negative distances, impossible measurements)\n- **Outliers** (data entry errors, measurement glitches)\n- **Inconsistent formats** (different units, encoding issues)\n\n### What We're Cleaning in Our Dataset\n\nFor radio signal data, we need to remove:\n\n1. **Missing values**: Rows where key features (distance, power, etc.) are missing\n   - Why? Can't predict without complete information\n   \n2. **Physically impossible values**:\n   - Distance â‰¤ 0: You can't have negative or zero distance\n   - Power (ERP) â‰¤ 0: Transmitters must have positive power\n   - Field strength < -100 dBu: Signals this weak are essentially noise\n   \n3. **Why these specific thresholds?**\n   - Radio engineering has physical constraints\n   - Values outside these ranges indicate measurement errors or data issues\n\n### The Impact\n\nData cleaning typically removes 1-10% of rows, but dramatically improves model quality. Better to have 900 clean examples than 1000 messy ones!\n\nLet's clean our data:",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Remove rows with missing values in our key columns\nml_cols = ['distance', 'field_strength', 'erp', 'frequency', 'haat']\ndf_clean = df.dropna(subset=ml_cols).copy()\n\n# Remove any rows with zero or negative values (physics doesn't allow these)\ndf_clean = df_clean[\n    (df_clean['distance'] > 0) & \n    (df_clean['erp'] > 0) & \n    (df_clean['field_strength'] > -100)  # Very weak signals can be negative in dBu\n]\n\nprint(f\"Clean dataset: {df_clean.shape[0]} rows (removed {df.shape[0] - df_clean.shape[0]} rows)\")\nprint(f\"\\nSearch locations represented: {df_clean['search_location'].nunique()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Understanding the Machine Learning Workflow\n",
    "\n",
    "Before we build our first model, let's understand the **5-step ML workflow** that we'll use throughout this lab. This is the standard process data scientists follow for most ML projects.\n",
    "\n",
    "## The 5 Steps Explained\n",
    "\n",
    "### Step 1: Prepare Features (X) and Target (y)\n",
    "\n",
    "**What is this?**  \n",
    "In ML, we need to separate our data into two parts:\n",
    "- **Features (X)**: The input variables we use to make predictions (also called \"independent variables\" or \"predictors\")\n",
    "- **Target (y)**: The output variable we want to predict (also called \"dependent variable\" or \"label\")\n",
    "\n",
    "**Analogy:** Think of predicting exam scores:\n",
    "- Features (X) might be: hours studied, hours slept, attendance rate\n",
    "- Target (y) would be: exam score\n",
    "\n",
    "**In our radio problem:**\n",
    "- Features (X): distance, power (ERP), frequency, antenna height\n",
    "- Target (y): field strength (the signal power we want to predict)\n",
    "\n",
    "**Why this matters:** The model learns the relationship between X and y. Good features = better predictions!\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Split into Training and Test Sets\n",
    "\n",
    "**What is this?**  \n",
    "We divide our dataset into two parts:\n",
    "- **Training set** (typically 70-80% of data): Used to teach the model\n",
    "- **Test set** (typically 20-30% of data): Used to evaluate the model on unseen data\n",
    "\n",
    "**Analogy:** It's like studying for an exam:\n",
    "- Training set = practice problems you study from\n",
    "- Test set = the actual exam (different questions testing the same concepts)\n",
    "\n",
    "**Why this matters:**  \n",
    "- If we test on data the model has already seen (training data), we can't tell if it truly learned patterns or just memorized answers\n",
    "- Testing on unseen data tells us how well the model will perform in the real world\n",
    "- This helps detect **overfitting** (when a model memorizes training data but fails on new data)\n",
    "\n",
    "**Important:** We use `random_state=42` to ensure everyone gets the same random split (reproducibility).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Train the Model\n",
    "\n",
    "**What is this?**  \n",
    "The model \"learns\" by finding mathematical patterns in the training data. For Linear Regression, it finds the best-fit line/plane/hyperplane through the data points.\n",
    "\n",
    "**Analogy:** Like learning to throw a forward pass in football:\n",
    "- You practice many throws during practice (training data)\n",
    "- Your brain learns the relationship between arm angle, velocity, receiver distance, and completion\n",
    "- After many reps, you can predict how to adjust your throw for different routes and distances\n",
    "- On game day, you apply what you learned to new situations\n",
    "\n",
    "![Eagles QB and WR](https://raw.githubusercontent.com/CSC-2053-100-Fall25/python-ml-template/main/aj-and-hurts.webp)\n",
    "\n",
    "**What happens internally:**  \n",
    "For Linear Regression, the model finds coefficients (weights) for each feature:\n",
    "```\n",
    "field_strength = (coefâ‚ Ã— distance) + (coefâ‚‚ Ã— power) + ... + intercept\n",
    "```\n",
    "\n",
    "The training process adjusts these coefficients to minimize prediction errors on the training set.\n",
    "\n",
    "**Why this matters:** This is where the \"machine learning\" happens! The model automatically discovers the mathematical relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Make Predictions\n",
    "\n",
    "**What is this?**  \n",
    "After training, we use the model to predict the target (y) for data it hasn't seen before (the test set).\n",
    "\n",
    "**Analogy:** After studying practice problems, you now take the actual exam and answer new questions.\n",
    "\n",
    "**In code:**\n",
    "```python\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**What the model does:**  \n",
    "It takes the test features and plugs them into the learned equation:\n",
    "```\n",
    "predicted_field_strength = (learned_coefâ‚ Ã— test_distance) + (learned_coefâ‚‚ Ã— test_power) + ...\n",
    "```\n",
    "\n",
    "**Why this matters:** Predictions are useless if we can't evaluate themâ€”that's next!\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Evaluate Performance\n",
    "\n",
    "**What is this?**  \n",
    "We compare the model's predictions to the actual values and calculate metrics to quantify how well it performed.\n",
    "\n",
    "**Common metrics for regression:**\n",
    "- **RÂ² (R-squared)**: Ranges from 0 to 1 (sometimes negative if the model is terrible)\n",
    "  - 1.0 = perfect predictions\n",
    "  - 0.5 = explains 50% of variance\n",
    "  - 0.0 = model is no better than predicting the average\n",
    "  \n",
    "- **RMSE (Root Mean Squared Error)**: Average prediction error in the same units as the target\n",
    "  - Lower is better\n",
    "  - If RMSE = 5 dBu, predictions are off by ~5 dBu on average\n",
    "  \n",
    "- **MAE (Mean Absolute Error)**: Average absolute error\n",
    "  - Easier to interpret than RMSE\n",
    "  - Less sensitive to outliers\n",
    "\n",
    "**Analogy:** After taking the exam:\n",
    "- You get your score (RÂ²)\n",
    "- You see how many points you missed on average per question (MAE)\n",
    "- You analyze which topics you need to study more (feature importance)\n",
    "\n",
    "**Why this matters:**  \n",
    "- Tells us if the model is good enough for real-world use\n",
    "- Helps us compare different models\n",
    "- Guides improvements (e.g., \"we need better features!\")\n",
    "\n",
    "---\n",
    "\n",
    "## Putting It All Together\n",
    "\n",
    "The workflow is **iterative**â€”you often repeat steps to improve:\n",
    "\n",
    "```\n",
    "Data â†’ Features/Target â†’ Split â†’ Train â†’ Predict â†’ Evaluate\n",
    "                          â†‘                            â†“\n",
    "                          â””â”€â”€â”€â”€â”€â”€â”€â”€ Improve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          (add features, try new models)\n",
    "```\n",
    "\n",
    "Now let's see this workflow in action!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 1: Simple Linear Regression\n",
    "## Predicting Field Strength from Distance Alone\n",
    "\n",
    "Let's start simple. Radio signals get weaker as you move farther from the transmitter. Can we predict field strength from distance alone?\n",
    "\n",
    "### The ML Workflow:\n",
    "1. **Prepare features (X) and target (y)**\n",
    "2. **Split into training and test sets**\n",
    "3. **Train the model**\n",
    "4. **Make predictions**\n",
    "5. **Evaluate performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare features and target\n",
    "X = df_clean[['distance']]  # Features (must be 2D for sklearn)\n",
    "y = df_clean['field_strength']  # Target (1D)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into training (80%) and test (20%) sets\n",
    "# random_state ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create and train the model\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained!\")\n",
    "print(f\"\\nLearned equation: field_strength = {model_1.coef_[0]:.4f} Ã— distance + {model_1.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Make predictions on test set\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "\n",
    "print(\"Predictions made on test set!\")\n",
    "print(f\"\\nFirst 5 predictions vs actual:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test.values[:5],\n",
    "    'Predicted': y_pred_1[:5],\n",
    "    'Error': y_test.values[:5] - y_pred_1[:5]\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "r2_1 = r2_score(y_test, y_pred_1)\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1))\n",
    "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
    "\n",
    "print(\"=== Model 1 Performance ===\")\n",
    "print(f\"RÂ² Score: {r2_1:.4f}\")\n",
    "print(f\"RMSE: {rmse_1:.2f} dBu\")\n",
    "print(f\"MAE: {mae_1:.2f} dBu\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- RÂ² = {r2_1:.2%} of variance in field strength is explained by distance\")\n",
    "print(f\"- On average, predictions are off by {mae_1:.1f} dBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_pred_1, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Field Strength (dBu)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Field Strength (dBu)', fontsize=12)\n",
    "axes[0].set_title(f'Model 1: Actual vs Predicted (RÂ² = {r2_1:.3f})', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Distance vs Field Strength with regression line\n",
    "axes[1].scatter(X_test, y_test, alpha=0.5, s=20, label='Actual')\n",
    "axes[1].plot(X_test, y_pred_1, 'r-', lw=2, label='Predicted', alpha=0.8)\n",
    "axes[1].set_xlabel('Distance (miles)', fontsize=12)\n",
    "axes[1].set_ylabel('Field Strength (dBu)', fontsize=12)\n",
    "axes[1].set_title('Distance vs Field Strength', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion\n",
    "- How well did distance alone predict field strength?\n",
    "- What does the RÂ² score tell us?\n",
    "- What other factors might affect signal strength?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Multiple Linear Regression\n",
    "## Adding More Features: Power, Frequency, and Height\n",
    "\n",
    "Radio signal strength depends on more than just distance:\n",
    "- **ERP (Effective Radiated Power)**: Higher power â†’ stronger signal\n",
    "- **Frequency**: Different frequencies propagate differently\n",
    "- **HAAT (Height Above Average Terrain)**: Higher antennas â†’ better coverage\n",
    "\n",
    "Let's add these features and see if our model improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features: now we use 4 features instead of 1\n",
    "feature_cols = ['distance', 'erp', 'frequency', 'haat']\n",
    "X_multi = df_clean[feature_cols]\n",
    "y_multi = df_clean['field_strength']\n",
    "\n",
    "# Split the data\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training with {len(feature_cols)} features: {feature_cols}\")\n",
    "print(f\"Training set: {X_train_m.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(X_train_m, y_train_m)\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "print(\"Feature Coefficients:\")\n",
    "for feature, coef in zip(feature_cols, model_2.coef_):\n",
    "    print(f\"  {feature:12s}: {coef:>10.4f}\")\n",
    "print(f\"  {'Intercept':12s}: {model_2.intercept_:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "y_pred_2 = model_2.predict(X_test_m)\n",
    "\n",
    "r2_2 = r2_score(y_test_m, y_pred_2)\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_test_m, y_pred_2))\n",
    "mae_2 = mean_absolute_error(y_test_m, y_pred_2)\n",
    "\n",
    "print(\"=== Model 2 Performance ===\")\n",
    "print(f\"RÂ² Score: {r2_2:.4f}\")\n",
    "print(f\"RMSE: {rmse_2:.2f} dBu\")\n",
    "print(f\"MAE: {mae_2:.2f} dBu\")\n",
    "print(\"\\n=== Comparison to Model 1 ===\")\n",
    "print(f\"RÂ² improvement: {r2_2 - r2_1:+.4f} ({(r2_2/r2_1 - 1)*100:+.1f}%)\")\n",
    "print(f\"RMSE improvement: {rmse_2 - rmse_1:+.2f} dBu ({(rmse_2/rmse_1 - 1)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model 2 performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].scatter(y_test_m, y_pred_2, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test_m.min(), y_test_m.max()], [y_test_m.min(), y_test_m.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Field Strength (dBu)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Field Strength (dBu)', fontsize=12)\n",
    "axes[0].set_title(f'Model 2: Actual vs Predicted (RÂ² = {r2_2:.3f})', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals (errors)\n",
    "residuals = y_test_m - y_pred_2\n",
    "axes[1].scatter(y_pred_2, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Field Strength (dBu)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion\n",
    "- Did adding more features improve the model?\n",
    "- Which feature has the largest coefficient? What does that mean?\n",
    "- Looking at the residual plot, are there any patterns? (Random residuals = good model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Feature Engineering\n",
    "## Creating New Features from Existing Ones\n",
    "\n",
    "In radio physics, signal strength decreases with the **square of distance** (inverse square law). Let's engineer some new features:\n",
    "- `distance_squared`: distanceÂ²\n",
    "- `log_distance`: log(distance) - many natural phenomena are logarithmic\n",
    "- `power_per_mile`: erp / distance - power \"density\"\n",
    "\n",
    "Feature engineering is often the key to better ML models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "df_engineered = df_clean.copy()\n",
    "\n",
    "df_engineered['distance_squared'] = df_engineered['distance'] ** 2\n",
    "df_engineered['log_distance'] = np.log(df_engineered['distance'] + 1)  # +1 to avoid log(0)\n",
    "df_engineered['log_erp'] = np.log(df_engineered['erp'] + 1)\n",
    "df_engineered['power_per_mile'] = df_engineered['erp'] / (df_engineered['distance'] + 1)\n",
    "\n",
    "print(\"New features created!\")\n",
    "print(df_engineered[['distance', 'distance_squared', 'log_distance', 'erp', 'log_erp', 'power_per_mile']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use original + engineered features\n",
    "feature_cols_eng = ['distance', 'erp', 'frequency', 'haat', \n",
    "                    'distance_squared', 'log_distance', 'log_erp', 'power_per_mile']\n",
    "\n",
    "X_eng = df_engineered[feature_cols_eng]\n",
    "y_eng = df_engineered['field_strength']\n",
    "\n",
    "# Split the data\n",
    "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(\n",
    "    X_eng, y_eng, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training with {len(feature_cols_eng)} features (including engineered ones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3\n",
    "model_3 = LinearRegression()\n",
    "model_3.fit(X_train_e, y_train_e)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_3 = model_3.predict(X_test_e)\n",
    "\n",
    "r2_3 = r2_score(y_test_e, y_pred_3)\n",
    "rmse_3 = np.sqrt(mean_squared_error(y_test_e, y_pred_3))\n",
    "mae_3 = mean_absolute_error(y_test_e, y_pred_3)\n",
    "\n",
    "print(\"=== Model 3 Performance (with engineered features) ===\")\n",
    "print(f\"RÂ² Score: {r2_3:.4f}\")\n",
    "print(f\"RMSE: {rmse_3:.2f} dBu\")\n",
    "print(f\"MAE: {mae_3:.2f} dBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Model 1: Distance Only', 'Model 2: Multiple Features', 'Model 3: + Engineering'],\n",
    "    'Features': [1, 4, 8],\n",
    "    'RÂ²': [r2_1, r2_2, r2_3],\n",
    "    'RMSE (dBu)': [rmse_1, rmse_2, rmse_3],\n",
    "    'MAE (dBu)': [mae_1, mae_2, mae_3]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance (absolute coefficient values)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols_eng,\n",
    "    'Coefficient': model_3.coef_,\n",
    "    'Abs_Coefficient': np.abs(model_3.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Feature Importance (Model 3 Coefficients)', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance[['Feature', 'Coefficient']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion\n",
    "- Did feature engineering improve the model?\n",
    "- Which engineered features were most important?\n",
    "- Can you think of other features we could create?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Exercise 1: Add Geographic Features\n",
    "\n",
    "Our dataset has latitude and longitude for both the transmitter (`lat`, `lon`) and receiver (`search_lat`, `search_lon`). \n",
    "\n",
    "**Your task:** Create a new model that includes bearing (direction) information. Does knowing the direction from transmitter to receiver improve predictions?\n",
    "\n",
    "**Hints:**\n",
    "- The `bearing` column already exists in the dataset\n",
    "- You could also try creating `sin(bearing)` and `cos(bearing)` features\n",
    "- Train a new model and compare to Model 3\n",
    "\n",
    "**Steps:**\n",
    "1. Create a new feature set that includes bearing-related features\n",
    "2. Split the data (use `random_state=42` for consistency)\n",
    "3. Train a LinearRegression model\n",
    "4. Evaluate and compare to Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Start by looking at the bearing column\n",
    "print(\"Bearing statistics:\")\n",
    "print(df_clean['bearing'].describe())\n",
    "\n",
    "# TODO: Create bearing features (sin and cos for cyclic nature of angles)\n",
    "# TODO: Combine with engineered features from Model 3\n",
    "# TODO: Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ¯ Exercise 2: Predict for a Specific Location\n",
    "\n",
    "Now let's use our best model to make real predictions!\n",
    "\n",
    "**Scenario:** You're planning to listen to the radio in **Philadelphia, PA** (39.9526Â°N, 75.1652Â°W). \n",
    "\n",
    "**Your task:**\n",
    "1. Filter the dataset to only include stations from Philadelphia searches\n",
    "2. Use Model 3 (or your improved model from Exercise 1) to predict field strength for these stations\n",
    "3. Find the top 10 stations with the strongest predicted signals\n",
    "4. Compare predictions to actual field strength values\n",
    "\n",
    "**Bonus:** Create a visualization showing predicted vs actual for Philadelphia stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Check which search_location names might contain Philadelphia\n",
    "print(\"Available search locations:\")\n",
    "print(df_clean['search_location'].unique())\n",
    "\n",
    "# TODO: Filter for Philadelphia (or closest location)\n",
    "# TODO: Create engineered features for this subset\n",
    "# TODO: Make predictions using model_3\n",
    "# TODO: Compare and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ“ Summary & Key Takeaways\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **The ML Workflow:**\n",
    "   - Load and clean data\n",
    "   - Prepare features (X) and target (y)\n",
    "   - Split into train/test sets\n",
    "   - Train model\n",
    "   - Evaluate and iterate\n",
    "\n",
    "2. **Regression Metrics:**\n",
    "   - **RÂ²**: Proportion of variance explained (higher is better, max = 1.0)\n",
    "   - **RMSE**: Root Mean Squared Error (lower is better, same units as target)\n",
    "   - **MAE**: Mean Absolute Error (average prediction error)\n",
    "\n",
    "3. **Feature Engineering Matters:**\n",
    "   - Simple models (1 feature) can give baseline performance\n",
    "   - Adding relevant features improves predictions\n",
    "   - Creating new features from domain knowledge (physics, geography) can boost performance significantly\n",
    "\n",
    "4. **scikit-learn Basics:**\n",
    "   ```python\n",
    "   from sklearn.linear_model import LinearRegression\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import r2_score, mean_squared_error\n",
    "   \n",
    "   model = LinearRegression()\n",
    "   model.fit(X_train, y_train)\n",
    "   predictions = model.predict(X_test)\n",
    "   ```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue learning:\n",
    "- Try other regression algorithms: Ridge, Lasso, RandomForest\n",
    "- Explore classification problems (predict station format from features)\n",
    "- Learn about cross-validation for more robust evaluation\n",
    "- Study regularization to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "### Questions for Reflection\n",
    "1. Why do we split data into training and test sets?\n",
    "2. What's the danger of using too many features?\n",
    "3. How would you explain RÂ² to a non-technical person?\n",
    "4. What real-world applications could use similar regression techniques?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}